use std::collections::HashMap;
use std::fs::File;
use std::io::{BufRead, BufReader, Write as IoWrite};
use std::sync::Arc;

use arrow::array::{
    ArrayRef, BooleanBuilder, Date32Builder, Float64Builder, Int64Builder, StringBuilder,
    TimestampMicrosecondBuilder,
};
use arrow::datatypes::{DataType as ArrowDataType, Field as ArrowField, Schema as ArrowSchema};
use arrow::record_batch::RecordBatch;
use chrono::{Datelike, NaiveDate, Utc};
use parquet::arrow::arrow_reader::ParquetRecordBatchReaderBuilder;
use parquet::arrow::arrow_writer::ArrowWriter;
use yachtsql_common::error::{Error, Result};
use yachtsql_common::types::{DataType, Value};
use yachtsql_ir::{ColumnDef, ExportFormat, ExportOptions, LoadFormat, LoadOptions};
use yachtsql_storage::{Field, Schema, Table};

use super::ConcurrentPlanExecutor;
use crate::plan::PhysicalPlan;

impl ConcurrentPlanExecutor<'_> {
    pub(crate) async fn execute_export(
        &self,
        options: &ExportOptions,
        query: &PhysicalPlan,
    ) -> Result<Table> {
        let data = self.execute_plan(query).await?;

        let is_cloud_uri = options.uri.starts_with("gs://")
            || options.uri.starts_with("s3://")
            || options.uri.starts_with("bigtable://")
            || options.uri.starts_with("pubsub://")
            || options.uri.starts_with("spanner://")
            || options.uri.contains("bigtable.googleapis.com")
            || options.uri.contains("pubsub.googleapis.com")
            || options.uri.contains("spanner.googleapis.com");

        if is_cloud_uri {
            return Ok(Table::empty(Schema::new()));
        }

        let path = if options.uri.starts_with("file://") {
            options.uri.strip_prefix("file://").unwrap().to_string()
        } else {
            options.uri.replace('*', "data")
        };

        match options.format {
            ExportFormat::Parquet => self.export_to_parquet(&data, &path),
            ExportFormat::Json => self.export_to_json(&data, &path),
            ExportFormat::Csv => self.export_to_csv(&data, &path, options),
            ExportFormat::Avro => Err(Error::UnsupportedFeature(
                "AVRO export not yet supported".into(),
            )),
        }
    }

    fn export_to_parquet(&self, data: &Table, path: &str) -> Result<Table> {
        let schema = data.schema();
        let num_rows = data.num_rows();

        let arrow_fields: Vec<ArrowField> = schema
            .fields()
            .iter()
            .map(|f| {
                let arrow_type = Self::data_type_to_arrow(&f.data_type);
                ArrowField::new(&f.name, arrow_type, f.is_nullable())
            })
            .collect();
        let arrow_schema = Arc::new(ArrowSchema::new(arrow_fields));

        let arrays = Self::build_arrow_arrays(data, schema, num_rows)?;

        let batch = RecordBatch::try_new(arrow_schema.clone(), arrays)
            .map_err(|e| Error::internal(format!("Failed to create RecordBatch: {}", e)))?;

        let file = File::create(path)
            .map_err(|e| Error::internal(format!("Failed to create file '{}': {}", path, e)))?;
        let mut writer = ArrowWriter::try_new(file, arrow_schema, None)
            .map_err(|e| Error::internal(format!("Failed to create Parquet writer: {}", e)))?;
        writer
            .write(&batch)
            .map_err(|e| Error::internal(format!("Failed to write Parquet data: {}", e)))?;
        writer
            .close()
            .map_err(|e| Error::internal(format!("Failed to close Parquet writer: {}", e)))?;

        Ok(Table::empty(Schema::new()))
    }

    fn export_to_json(&self, data: &Table, path: &str) -> Result<Table> {
        let schema = data.schema();
        let mut file = File::create(path)
            .map_err(|e| Error::internal(format!("Failed to create file '{}': {}", path, e)))?;

        for record in data.rows()? {
            let mut obj = serde_json::Map::new();
            for (i, field) in schema.fields().iter().enumerate() {
                let val = &record.values()[i];
                obj.insert(field.name.clone(), Self::value_to_json(val));
            }
            let line = serde_json::to_string(&obj)
                .map_err(|e| Error::internal(format!("JSON serialization error: {}", e)))?;
            writeln!(file, "{}", line)
                .map_err(|e| Error::internal(format!("Failed to write JSON: {}", e)))?;
        }

        Ok(Table::empty(Schema::new()))
    }

    fn export_to_csv(&self, data: &Table, path: &str, options: &ExportOptions) -> Result<Table> {
        let schema = data.schema();
        let delimiter = options
            .field_delimiter
            .as_ref()
            .and_then(|d| d.chars().next())
            .unwrap_or(',');

        let mut file = File::create(path)
            .map_err(|e| Error::internal(format!("Failed to create file '{}': {}", path, e)))?;

        if options.header.unwrap_or(false) {
            let header: Vec<&str> = schema.fields().iter().map(|f| f.name.as_str()).collect();
            writeln!(file, "{}", header.join(&delimiter.to_string()))
                .map_err(|e| Error::internal(format!("Failed to write CSV header: {}", e)))?;
        }

        for record in data.rows()? {
            let values: Vec<String> = record
                .values()
                .iter()
                .map(Self::value_to_csv_string)
                .collect();
            writeln!(file, "{}", values.join(&delimiter.to_string()))
                .map_err(|e| Error::internal(format!("Failed to write CSV row: {}", e)))?;
        }

        Ok(Table::empty(Schema::new()))
    }

    fn value_to_json(val: &Value) -> serde_json::Value {
        match val {
            Value::Null => serde_json::Value::Null,
            Value::Bool(b) => serde_json::Value::Bool(*b),
            Value::Int64(n) => serde_json::Value::Number((*n).into()),
            Value::Float64(f) => serde_json::json!(f.0),
            Value::String(s) => serde_json::Value::String(s.clone()),
            Value::Date(d) => serde_json::Value::String(d.to_string()),
            Value::DateTime(dt) => serde_json::Value::String(dt.to_string()),
            Value::Timestamp(ts) => serde_json::Value::String(ts.to_rfc3339()),
            Value::Time(t) => serde_json::Value::String(t.to_string()),
            Value::Numeric(n) => serde_json::Value::String(n.to_string()),
            Value::BigNumeric(n) => serde_json::Value::String(n.to_string()),
            Value::Array(arr) => {
                serde_json::Value::Array(arr.iter().map(Self::value_to_json).collect())
            }
            Value::Struct(fields) => {
                let obj: serde_json::Map<String, serde_json::Value> = fields
                    .iter()
                    .map(|(k, v)| (k.clone(), Self::value_to_json(v)))
                    .collect();
                serde_json::Value::Object(obj)
            }
            Value::Json(j) => j.clone(),
            Value::Bytes(_)
            | Value::Geography(_)
            | Value::Interval(_)
            | Value::Range(_)
            | Value::Default => serde_json::Value::String(format!("{:?}", val)),
        }
    }

    fn value_to_csv_string(val: &Value) -> String {
        match val {
            Value::Null => String::new(),
            Value::Bool(b) => b.to_string(),
            Value::Int64(n) => n.to_string(),
            Value::Float64(f) => f.0.to_string(),
            Value::String(s) => {
                if s.contains(',') || s.contains('"') || s.contains('\n') {
                    format!("\"{}\"", s.replace('"', "\"\""))
                } else {
                    s.clone()
                }
            }
            Value::Date(d) => d.to_string(),
            Value::DateTime(dt) => dt.to_string(),
            Value::Timestamp(ts) => ts.to_rfc3339(),
            Value::Time(t) => t.to_string(),
            Value::Numeric(n) => n.to_string(),
            Value::BigNumeric(n) => n.to_string(),
            Value::Bytes(_)
            | Value::Json(_)
            | Value::Array(_)
            | Value::Struct(_)
            | Value::Geography(_)
            | Value::Interval(_)
            | Value::Range(_)
            | Value::Default => format!("{:?}", val),
        }
    }

    fn data_type_to_arrow(data_type: &DataType) -> ArrowDataType {
        match data_type {
            DataType::Bool => ArrowDataType::Boolean,
            DataType::Int64 => ArrowDataType::Int64,
            DataType::Float64 => ArrowDataType::Float64,
            DataType::String => ArrowDataType::Utf8,
            DataType::Date => ArrowDataType::Date32,
            DataType::DateTime => {
                ArrowDataType::Timestamp(arrow::datatypes::TimeUnit::Microsecond, None)
            }
            DataType::Timestamp => ArrowDataType::Timestamp(
                arrow::datatypes::TimeUnit::Microsecond,
                Some("UTC".into()),
            ),
            DataType::Time => ArrowDataType::Time64(arrow::datatypes::TimeUnit::Microsecond),
            DataType::Bytes => ArrowDataType::Binary,
            DataType::Numeric(_) => ArrowDataType::Utf8,
            DataType::Unknown
            | DataType::BigNumeric
            | DataType::Geography
            | DataType::Json
            | DataType::Struct(_)
            | DataType::Array(_)
            | DataType::Interval
            | DataType::Range(_) => ArrowDataType::Utf8,
        }
    }

    fn build_arrow_arrays(
        data: &Table,
        schema: &Schema,
        _num_rows: usize,
    ) -> Result<Vec<ArrayRef>> {
        let mut arrays: Vec<ArrayRef> = Vec::new();

        for (col_idx, field) in schema.fields().iter().enumerate() {
            let array: ArrayRef = match &field.data_type {
                DataType::Bool => {
                    let mut builder = BooleanBuilder::new();
                    for record in data.rows()? {
                        let val = &record.values()[col_idx];
                        if let Value::Bool(b) = val {
                            builder.append_value(*b);
                        } else {
                            builder.append_null();
                        }
                    }
                    Arc::new(builder.finish())
                }
                DataType::Int64 => {
                    let mut builder = Int64Builder::new();
                    for record in data.rows()? {
                        let val = &record.values()[col_idx];
                        match val {
                            Value::Int64(n) => builder.append_value(*n),
                            Value::Float64(f) => builder.append_value(f.0 as i64),
                            Value::Null
                            | Value::Bool(_)
                            | Value::Numeric(_)
                            | Value::BigNumeric(_)
                            | Value::String(_)
                            | Value::Bytes(_)
                            | Value::Date(_)
                            | Value::Time(_)
                            | Value::DateTime(_)
                            | Value::Timestamp(_)
                            | Value::Json(_)
                            | Value::Array(_)
                            | Value::Struct(_)
                            | Value::Geography(_)
                            | Value::Interval(_)
                            | Value::Range(_)
                            | Value::Default => builder.append_null(),
                        }
                    }
                    Arc::new(builder.finish())
                }
                DataType::Float64 => {
                    let mut builder = Float64Builder::new();
                    for record in data.rows()? {
                        let val = &record.values()[col_idx];
                        match val {
                            Value::Float64(f) => builder.append_value(f.0),
                            Value::Int64(n) => builder.append_value(*n as f64),
                            Value::Null
                            | Value::Bool(_)
                            | Value::Numeric(_)
                            | Value::BigNumeric(_)
                            | Value::String(_)
                            | Value::Bytes(_)
                            | Value::Date(_)
                            | Value::Time(_)
                            | Value::DateTime(_)
                            | Value::Timestamp(_)
                            | Value::Json(_)
                            | Value::Array(_)
                            | Value::Struct(_)
                            | Value::Geography(_)
                            | Value::Interval(_)
                            | Value::Range(_)
                            | Value::Default => builder.append_null(),
                        }
                    }
                    Arc::new(builder.finish())
                }
                DataType::String
                | DataType::Unknown
                | DataType::Numeric(_)
                | DataType::BigNumeric
                | DataType::Json
                | DataType::Geography
                | DataType::Interval
                | DataType::Range(_)
                | DataType::Struct(_)
                | DataType::Array(_) => {
                    let mut builder = StringBuilder::new();
                    for record in data.rows()? {
                        let val = &record.values()[col_idx];
                        if val.is_null() {
                            builder.append_null();
                        } else {
                            builder.append_value(format!("{}", val));
                        }
                    }
                    Arc::new(builder.finish())
                }
                DataType::Date => {
                    let mut builder = Date32Builder::new();
                    for record in data.rows()? {
                        let val = &record.values()[col_idx];
                        match val {
                            Value::Date(d) => {
                                let epoch = NaiveDate::from_ymd_opt(1970, 1, 1).unwrap();
                                let days = d.signed_duration_since(epoch).num_days() as i32;
                                builder.append_value(days);
                            }
                            Value::Null
                            | Value::Bool(_)
                            | Value::Int64(_)
                            | Value::Float64(_)
                            | Value::Numeric(_)
                            | Value::BigNumeric(_)
                            | Value::String(_)
                            | Value::Bytes(_)
                            | Value::Time(_)
                            | Value::DateTime(_)
                            | Value::Timestamp(_)
                            | Value::Json(_)
                            | Value::Array(_)
                            | Value::Struct(_)
                            | Value::Geography(_)
                            | Value::Interval(_)
                            | Value::Range(_)
                            | Value::Default => builder.append_null(),
                        }
                    }
                    Arc::new(builder.finish())
                }
                DataType::DateTime | DataType::Timestamp => {
                    let mut builder = TimestampMicrosecondBuilder::new();
                    for record in data.rows()? {
                        let val = &record.values()[col_idx];
                        match val {
                            Value::DateTime(dt) => {
                                let micros = dt.and_utc().timestamp_micros();
                                builder.append_value(micros);
                            }
                            Value::Timestamp(ts) => {
                                let micros = ts.timestamp_micros();
                                builder.append_value(micros);
                            }
                            Value::Null
                            | Value::Bool(_)
                            | Value::Int64(_)
                            | Value::Float64(_)
                            | Value::Numeric(_)
                            | Value::BigNumeric(_)
                            | Value::String(_)
                            | Value::Bytes(_)
                            | Value::Date(_)
                            | Value::Time(_)
                            | Value::Json(_)
                            | Value::Array(_)
                            | Value::Struct(_)
                            | Value::Geography(_)
                            | Value::Interval(_)
                            | Value::Range(_)
                            | Value::Default => builder.append_null(),
                        }
                    }
                    Arc::new(builder.finish())
                }
                DataType::Time | DataType::Bytes => {
                    let mut builder = StringBuilder::new();
                    for record in data.rows()? {
                        let val = &record.values()[col_idx];
                        if val.is_null() {
                            builder.append_null();
                        } else {
                            builder.append_value(format!("{}", val));
                        }
                    }
                    Arc::new(builder.finish())
                }
            };
            arrays.push(array);
        }

        Ok(arrays)
    }

    pub(crate) fn execute_load(
        &self,
        table_name: &str,
        options: &LoadOptions,
        temp_table: bool,
        temp_schema: Option<&Vec<ColumnDef>>,
    ) -> Result<Table> {
        if temp_table && let Some(col_defs) = temp_schema {
            let fields: Vec<Field> = col_defs
                .iter()
                .map(|col| Field::nullable(col.name.clone(), col.data_type.clone()))
                .collect();
            let schema = Schema::from_fields(fields);
            let _ = self.catalog.create_table(table_name, schema);
            let handle = self
                .catalog
                .get_table_handle(table_name)
                .ok_or_else(|| Error::TableNotFound(table_name.to_string()))?;
            {
                let table = handle.write().clone();
                self.tables
                    .add_write_table(table_name.to_uppercase(), table);
            }
        }

        let schema = self
            .tables
            .get_table(table_name)
            .ok_or_else(|| Error::TableNotFound(table_name.to_string()))?
            .schema()
            .clone();

        if options.overwrite
            && let Some(t) = self.tables.get_table_mut(table_name)
        {
            t.clear();
        }

        for uri in &options.uris {
            let (path, is_cloud_uri) = if uri.starts_with("file://") {
                (uri.strip_prefix("file://").unwrap().to_string(), false)
            } else if uri.starts_with("gs://") {
                (
                    uri.strip_prefix("gs://").unwrap().replace('*', "data"),
                    true,
                )
            } else if uri.starts_with("s3://") {
                (
                    uri.strip_prefix("s3://").unwrap().replace('*', "data"),
                    true,
                )
            } else {
                (uri.clone(), false)
            };

            if is_cloud_uri && !std::path::Path::new(&path).exists() {
                continue;
            }

            let rows = match options.format {
                LoadFormat::Parquet => self.load_parquet(&path, &schema)?,
                LoadFormat::Json => self.load_json(&path, &schema)?,
                LoadFormat::Csv => self.load_csv(&path, &schema, options)?,
                LoadFormat::Avro => {
                    return Err(Error::UnsupportedFeature(
                        "AVRO load not yet supported".into(),
                    ));
                }
            };

            let table = self
                .tables
                .get_table_mut(table_name)
                .ok_or_else(|| Error::TableNotFound(table_name.to_string()))?;

            for row in rows {
                table.push_row(row)?;
            }
        }

        Ok(Table::empty(Schema::new()))
    }

    fn load_parquet(&self, path: &str, schema: &Schema) -> Result<Vec<Vec<Value>>> {
        use arrow::array::AsArray;

        let file = File::open(path)
            .map_err(|e| Error::internal(format!("Failed to open file '{}': {}", path, e)))?;

        let reader = ParquetRecordBatchReaderBuilder::try_new(file)
            .map_err(|e| Error::internal(format!("Failed to read Parquet: {}", e)))?
            .build()
            .map_err(|e| Error::internal(format!("Failed to build Parquet reader: {}", e)))?;

        let mut rows = Vec::new();
        let target_columns: Vec<String> = schema.fields().iter().map(|f| f.name.clone()).collect();
        let target_types: Vec<DataType> = schema
            .fields()
            .iter()
            .map(|f| f.data_type.clone())
            .collect();

        for batch_result in reader {
            let batch = batch_result
                .map_err(|e| Error::internal(format!("Failed to read batch: {}", e)))?;

            let parquet_schema = batch.schema();
            let parquet_columns: Vec<String> = parquet_schema
                .fields()
                .iter()
                .map(|f| f.name().to_string())
                .collect();

            let column_mapping: Vec<Option<usize>> = target_columns
                .iter()
                .map(|target_col| {
                    parquet_columns
                        .iter()
                        .position(|pc| pc.eq_ignore_ascii_case(target_col))
                })
                .collect();

            for row_idx in 0..batch.num_rows() {
                let mut row_values = Vec::with_capacity(target_columns.len());
                for (col_idx, parquet_col_idx) in column_mapping.iter().enumerate() {
                    let value = match parquet_col_idx {
                        Some(pci) => {
                            let array = batch.column(*pci);
                            self.arrow_array_to_value(array, row_idx, &target_types[col_idx])?
                        }
                        None => Value::null(),
                    };
                    row_values.push(value);
                }
                rows.push(row_values);
            }
        }

        Ok(rows)
    }

    fn arrow_array_to_value(
        &self,
        array: &ArrayRef,
        row_idx: usize,
        target_type: &DataType,
    ) -> Result<Value> {
        use arrow::array::AsArray;

        if array.is_null(row_idx) {
            return Ok(Value::null());
        }

        let value = match array.data_type() {
            ArrowDataType::Boolean => {
                let arr = array.as_boolean();
                Value::bool_val(arr.value(row_idx))
            }
            ArrowDataType::Int64 => {
                let arr = array.as_primitive::<arrow::datatypes::Int64Type>();
                Value::int64(arr.value(row_idx))
            }
            ArrowDataType::Int32 => {
                let arr = array.as_primitive::<arrow::datatypes::Int32Type>();
                Value::int64(arr.value(row_idx) as i64)
            }
            ArrowDataType::Float64 => {
                let arr = array.as_primitive::<arrow::datatypes::Float64Type>();
                Value::float64(arr.value(row_idx))
            }
            ArrowDataType::Float32 => {
                let arr = array.as_primitive::<arrow::datatypes::Float32Type>();
                Value::float64(arr.value(row_idx) as f64)
            }
            ArrowDataType::Utf8 => {
                let arr = array.as_string::<i32>();
                Value::string(arr.value(row_idx).to_string())
            }
            ArrowDataType::LargeUtf8 => {
                let arr = array.as_string::<i64>();
                Value::string(arr.value(row_idx).to_string())
            }
            ArrowDataType::Date32 => {
                let arr = array.as_primitive::<arrow::datatypes::Date32Type>();
                let days = arr.value(row_idx);
                let date =
                    chrono::NaiveDate::from_num_days_from_ce_opt(days + 719163).unwrap_or_default();
                Value::date(date)
            }
            ArrowDataType::Date64 => {
                let arr = array.as_primitive::<arrow::datatypes::Date64Type>();
                let millis = arr.value(row_idx);
                let date = chrono::DateTime::from_timestamp_millis(millis)
                    .map(|dt| dt.naive_utc().date())
                    .unwrap_or_default();
                Value::date(date)
            }
            ArrowDataType::Timestamp(_, _) => {
                let arr = array.as_primitive::<arrow::datatypes::TimestampMicrosecondType>();
                let micros = arr.value(row_idx);
                match target_type {
                    DataType::DateTime => {
                        let dt = chrono::DateTime::from_timestamp_micros(micros)
                            .map(|dt| dt.naive_utc())
                            .unwrap_or_default();
                        Value::datetime(dt)
                    }
                    _ => {
                        let ts =
                            chrono::DateTime::from_timestamp_micros(micros).unwrap_or_default();
                        Value::timestamp(ts)
                    }
                }
            }
            _ => Value::string(format!("{:?}", array)),
        };

        Ok(value)
    }

    fn load_json(&self, path: &str, schema: &Schema) -> Result<Vec<Vec<Value>>> {
        let file = File::open(path)
            .map_err(|e| Error::internal(format!("Failed to open file '{}': {}", path, e)))?;
        let reader = BufReader::new(file);

        let target_columns: Vec<String> = schema.fields().iter().map(|f| f.name.clone()).collect();
        let target_types: Vec<DataType> = schema
            .fields()
            .iter()
            .map(|f| f.data_type.clone())
            .collect();

        let mut rows = Vec::new();

        for line in reader.lines() {
            let line =
                line.map_err(|e| Error::internal(format!("Failed to read JSON line: {}", e)))?;
            let trimmed = line.trim();
            if trimmed.is_empty() {
                continue;
            }

            let obj: HashMap<String, serde_json::Value> = serde_json::from_str(trimmed)
                .map_err(|e| Error::internal(format!("Failed to parse JSON: {}", e)))?;

            let mut row_values = Vec::with_capacity(target_columns.len());

            for (i, col_name) in target_columns.iter().enumerate() {
                let json_val = obj
                    .iter()
                    .find(|(k, _)| k.eq_ignore_ascii_case(col_name))
                    .map(|(_, v)| v);
                let value = match json_val {
                    Some(v) => self.json_to_value(v, &target_types[i])?,
                    None => Value::null(),
                };
                row_values.push(value);
            }

            rows.push(row_values);
        }

        Ok(rows)
    }

    fn json_to_value(&self, json_val: &serde_json::Value, target_type: &DataType) -> Result<Value> {
        match json_val {
            serde_json::Value::Null => Ok(Value::null()),
            serde_json::Value::Bool(b) => Ok(Value::bool_val(*b)),
            serde_json::Value::Number(n) => {
                if let Some(i) = n.as_i64() {
                    match target_type {
                        DataType::Float64 => Ok(Value::float64(i as f64)),
                        _ => Ok(Value::int64(i)),
                    }
                } else if let Some(f) = n.as_f64() {
                    Ok(Value::float64(f))
                } else {
                    Ok(Value::null())
                }
            }
            serde_json::Value::String(s) => match target_type {
                DataType::Date => {
                    let date = chrono::NaiveDate::parse_from_str(s, "%Y-%m-%d")
                        .map_err(|e| Error::internal(format!("Invalid date: {}", e)))?;
                    Ok(Value::date(date))
                }
                DataType::DateTime => {
                    let dt = chrono::NaiveDateTime::parse_from_str(s, "%Y-%m-%d %H:%M:%S")
                        .or_else(|_| chrono::NaiveDateTime::parse_from_str(s, "%Y-%m-%dT%H:%M:%S"))
                        .map_err(|e| Error::internal(format!("Invalid datetime: {}", e)))?;
                    Ok(Value::datetime(dt))
                }
                DataType::Timestamp => {
                    let ts = chrono::DateTime::parse_from_rfc3339(s)
                        .map(|d| d.with_timezone(&Utc))
                        .map_err(|e| Error::internal(format!("Invalid timestamp: {}", e)))?;
                    Ok(Value::timestamp(ts))
                }
                _ => Ok(Value::string(s.clone())),
            },
            serde_json::Value::Array(arr) => {
                let values: Result<Vec<Value>> = arr
                    .iter()
                    .map(|v| self.json_to_value(v, &DataType::Unknown))
                    .collect();
                Ok(Value::array(values?))
            }
            serde_json::Value::Object(obj) => {
                let fields: Result<Vec<(String, Value)>> = obj
                    .iter()
                    .map(|(k, v)| {
                        let val = self.json_to_value(v, &DataType::Unknown)?;
                        Ok((k.clone(), val))
                    })
                    .collect();
                Ok(Value::Struct(fields?))
            }
        }
    }

    fn load_csv(
        &self,
        path: &str,
        schema: &Schema,
        options: &LoadOptions,
    ) -> Result<Vec<Vec<Value>>> {
        let file = File::open(path)
            .map_err(|e| Error::internal(format!("Failed to open file '{}': {}", path, e)))?;
        let reader = BufReader::new(file);

        let target_types: Vec<DataType> = schema
            .fields()
            .iter()
            .map(|f| f.data_type.clone())
            .collect();

        let delimiter = options
            .field_delimiter
            .as_ref()
            .and_then(|d| {
                if d == "\\t" || d == "\t" {
                    Some('\t')
                } else {
                    d.chars().next()
                }
            })
            .unwrap_or(',');

        let null_marker = options.null_marker.as_deref();
        let skip_rows = options.skip_leading_rows.unwrap_or(0) as usize;

        let mut rows = Vec::new();
        let lines = reader.lines();

        for (idx, line) in lines.enumerate() {
            if idx < skip_rows {
                continue;
            }

            let line =
                line.map_err(|e| Error::internal(format!("Failed to read CSV line: {}", e)))?;
            let trimmed = line.trim();
            if trimmed.is_empty() {
                continue;
            }

            let parts = Self::parse_csv_line(trimmed, delimiter);
            let mut row_values = Vec::with_capacity(target_types.len());

            for (i, part) in parts.iter().enumerate() {
                if i >= target_types.len() {
                    break;
                }
                let value = if null_marker.is_some_and(|nm| part == nm) {
                    Value::null()
                } else {
                    self.csv_string_to_value(part, &target_types[i])?
                };
                row_values.push(value);
            }

            while row_values.len() < target_types.len() {
                row_values.push(Value::null());
            }

            rows.push(row_values);
        }

        Ok(rows)
    }

    fn parse_csv_line(line: &str, delimiter: char) -> Vec<String> {
        let mut fields = Vec::new();
        let mut current = String::new();
        let mut in_quotes = false;
        let mut chars = line.chars().peekable();

        while let Some(c) = chars.next() {
            if c == '"' {
                if in_quotes {
                    if chars.peek() == Some(&'"') {
                        chars.next();
                        current.push('"');
                    } else {
                        in_quotes = false;
                    }
                } else {
                    in_quotes = true;
                }
            } else if c == delimiter && !in_quotes {
                fields.push(current.trim().to_string());
                current = String::new();
            } else {
                current.push(c);
            }
        }
        fields.push(current.trim().to_string());
        fields
    }

    fn csv_string_to_value(&self, s: &str, target_type: &DataType) -> Result<Value> {
        if s.is_empty() {
            return Ok(Value::null());
        }

        match target_type {
            DataType::Bool => {
                let b = matches!(s.to_uppercase().as_str(), "TRUE" | "1" | "YES");
                Ok(Value::bool_val(b))
            }
            DataType::Int64 => {
                let n = s
                    .parse::<i64>()
                    .map_err(|_| Error::internal(format!("Invalid integer: {}", s)))?;
                Ok(Value::int64(n))
            }
            DataType::Float64 => {
                let f = s
                    .parse::<f64>()
                    .map_err(|_| Error::internal(format!("Invalid float: {}", s)))?;
                Ok(Value::float64(f))
            }
            DataType::Date => {
                let date = chrono::NaiveDate::parse_from_str(s, "%Y-%m-%d")
                    .map_err(|e| Error::internal(format!("Invalid date '{}': {}", s, e)))?;
                Ok(Value::date(date))
            }
            DataType::DateTime => {
                let dt = chrono::NaiveDateTime::parse_from_str(s, "%Y-%m-%d %H:%M:%S")
                    .or_else(|_| chrono::NaiveDateTime::parse_from_str(s, "%Y-%m-%dT%H:%M:%S"))
                    .map_err(|e| Error::internal(format!("Invalid datetime '{}': {}", s, e)))?;
                Ok(Value::datetime(dt))
            }
            DataType::Timestamp => {
                let ts = chrono::DateTime::parse_from_rfc3339(s)
                    .map(|d| d.with_timezone(&Utc))
                    .or_else(|_| {
                        chrono::NaiveDateTime::parse_from_str(s, "%Y-%m-%d %H:%M:%S")
                            .or_else(|_| {
                                chrono::NaiveDateTime::parse_from_str(s, "%Y-%m-%dT%H:%M:%S")
                            })
                            .map(|dt| dt.and_utc())
                    })
                    .map_err(|e| Error::internal(format!("Invalid timestamp '{}': {}", s, e)))?;
                Ok(Value::timestamp(ts))
            }
            _ => Ok(Value::string(s.to_string())),
        }
    }
}
